{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ccab467",
   "metadata": {},
   "source": [
    "# **CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b060278",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db1d9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bascic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Model imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# More robust model imports\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1778d1",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0e046928",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/cleaned/data_final.csv')\n",
    "\n",
    "# We drop the columns that we won't use for classification\n",
    "data = data.drop(columns=[\"url\", \"description\"])\n",
    "\n",
    "# Convertimos las columnas de texto a categorías\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# data.info()\n",
    "# data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc9213",
   "metadata": {},
   "source": [
    "## Based on Zone\n",
    "\n",
    "First we will try to classify the houses according to their zone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e0aaf",
   "metadata": {},
   "source": [
    "### Modify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6affda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the neighbourhood column to classify by zone\n",
    "data_zone = data.drop(columns=[\"neighborhood\"])\n",
    "\n",
    "# We also split the data into features and target\n",
    "X_zone = data_zone.drop(columns=[\"zone\"])\n",
    "y_zone = data_zone[\"zone\"]\n",
    "\n",
    "# Check everything is ok\n",
    "#print(y_zone.head())\n",
    "#X_zone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b953769",
   "metadata": {},
   "source": [
    "### Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e68785a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas para evaluación (ajustadas para clasificación multiclase)\n",
    "SCORING = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'recall_weighted': 'recall_weighted',\n",
    "    'precision_weighted': 'precision_weighted',\n",
    "    'f1_weighted': 'f1_weighted',\n",
    "    'f1_macro': 'f1_macro',       # CRÍTICO: Te dirá si estás fallando en las clases pequeñas\n",
    "    'matthews': \"matthews_corrcoef\",\n",
    "    'balanced_accuracy': 'balanced_accuracy'\n",
    "}\n",
    "\n",
    "# Probar múltiples modelos\n",
    "MODELS = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest (tuned)': RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_split=5, random_state=42, n_jobs=-1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial'),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "#    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True),\n",
    "#    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingClassifier(max_iter=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=0, allow_writing_files=False), # (Verbose=0 para que no llene la pantalla de logs)\n",
    "    'Neural Network (MLP)': make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "}\n",
    "\n",
    "# Usar StratifiedKFold para mantener la distribución de clases\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfbd8f",
   "metadata": {},
   "source": [
    "### Defining function to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c17fe8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, y, cv_strategy=cv_strategy, scoring=SCORING, models=MODELS):\n",
    "    # We are going to try all defined models\n",
    "    results = {}\n",
    "    # We evaluate each model using cross-validation\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        scores = cross_validate(model, X, y, cv=cv_strategy, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "        \n",
    "        # We check the results with several metrics\n",
    "        results[name] = {}\n",
    "        for metric_name, metric_scores in scores.items():\n",
    "            if metric_name.startswith('test_'):\n",
    "                metric = metric_name.replace('test_', '')\n",
    "                results[name][metric] = (metric_scores.mean(), metric_scores.std())\n",
    "                print(f\"  {metric}: {metric_scores.mean():.4f} (+/- {metric_scores.std():.4f})\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb003bb",
   "metadata": {},
   "source": [
    "### Strategy 1\n",
    "Use all the variables, encoding categorical ones as dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da5a8f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ESTRATEGIA 1: Utilizar todas las variables, codificando categóricas con dummies\n",
      "================================================================================\n",
      "\n",
      "Número de muestras: 1230\n",
      "Número de features: 324\n",
      "Distribución de clases: [237 127  76  61  90 156 136  34 167  33 113]\n",
      "\n",
      "Random Forest:\n",
      "  accuracy: 0.6073 (+/- 0.0225)\n",
      "  recall_weighted: 0.6073 (+/- 0.0225)\n",
      "  precision_weighted: 0.6346 (+/- 0.0297)\n",
      "  f1_weighted: 0.5966 (+/- 0.0253)\n",
      "  f1_macro: 0.5697 (+/- 0.0380)\n",
      "  matthews: 0.5531 (+/- 0.0262)\n",
      "  balanced_accuracy: 0.5439 (+/- 0.0370)\n",
      "\n",
      "Random Forest (tuned):\n",
      "  accuracy: 0.5707 (+/- 0.0186)\n",
      "  recall_weighted: 0.5707 (+/- 0.0186)\n",
      "  precision_weighted: 0.6337 (+/- 0.0337)\n",
      "  f1_weighted: 0.5493 (+/- 0.0209)\n",
      "  f1_macro: 0.5133 (+/- 0.0309)\n",
      "  matthews: 0.5133 (+/- 0.0213)\n",
      "  balanced_accuracy: 0.4814 (+/- 0.0293)\n",
      "\n",
      "Logistic Regression:\n",
      "  accuracy: 0.2976 (+/- 0.0167)\n",
      "  recall_weighted: 0.2976 (+/- 0.0167)\n",
      "  precision_weighted: 0.1599 (+/- 0.0158)\n",
      "  f1_weighted: 0.1957 (+/- 0.0071)\n",
      "  f1_macro: 0.1168 (+/- 0.0055)\n",
      "  matthews: 0.1862 (+/- 0.0223)\n",
      "  balanced_accuracy: 0.1768 (+/- 0.0092)\n",
      "\n",
      "Naive Bayes:\n",
      "  accuracy: 0.3317 (+/- 0.0208)\n",
      "  recall_weighted: 0.3317 (+/- 0.0208)\n",
      "  precision_weighted: 0.3318 (+/- 0.0481)\n",
      "  f1_weighted: 0.2834 (+/- 0.0243)\n",
      "  f1_macro: 0.2123 (+/- 0.0198)\n",
      "  matthews: 0.2463 (+/- 0.0258)\n",
      "  balanced_accuracy: 0.2459 (+/- 0.0209)\n",
      "\n",
      "Gradient Boosting:\n",
      "  accuracy: 0.5789 (+/- 0.0285)\n",
      "  recall_weighted: 0.5789 (+/- 0.0285)\n",
      "  precision_weighted: 0.5960 (+/- 0.0346)\n",
      "  f1_weighted: 0.5713 (+/- 0.0289)\n",
      "  f1_macro: 0.5425 (+/- 0.0405)\n",
      "  matthews: 0.5218 (+/- 0.0325)\n",
      "  balanced_accuracy: 0.5240 (+/- 0.0398)\n",
      "\n",
      "Hist Gradient Boosting:\n",
      "  accuracy: 0.6439 (+/- 0.0179)\n",
      "  recall_weighted: 0.6439 (+/- 0.0179)\n",
      "  precision_weighted: 0.6500 (+/- 0.0162)\n",
      "  f1_weighted: 0.6425 (+/- 0.0177)\n",
      "  f1_macro: 0.6253 (+/- 0.0222)\n",
      "  matthews: 0.5965 (+/- 0.0200)\n",
      "  balanced_accuracy: 0.6118 (+/- 0.0234)\n",
      "\n",
      "XGBoost:\n",
      "  accuracy: 0.6634 (+/- 0.0104)\n",
      "  recall_weighted: 0.6634 (+/- 0.0104)\n",
      "  precision_weighted: 0.6713 (+/- 0.0135)\n",
      "  f1_weighted: 0.6630 (+/- 0.0112)\n",
      "  f1_macro: 0.6572 (+/- 0.0121)\n",
      "  matthews: 0.6187 (+/- 0.0116)\n",
      "  balanced_accuracy: 0.6436 (+/- 0.0097)\n",
      "\n",
      "LightGBM:\n",
      "  accuracy: 0.6268 (+/- 0.0214)\n",
      "  recall_weighted: 0.6268 (+/- 0.0214)\n",
      "  precision_weighted: 0.6378 (+/- 0.0190)\n",
      "  f1_weighted: 0.6244 (+/- 0.0191)\n",
      "  f1_macro: 0.6030 (+/- 0.0197)\n",
      "  matthews: 0.5772 (+/- 0.0237)\n",
      "  balanced_accuracy: 0.5892 (+/- 0.0228)\n",
      "\n",
      "CatBoost:\n",
      "  accuracy: 0.6049 (+/- 0.0107)\n",
      "  recall_weighted: 0.6049 (+/- 0.0107)\n",
      "  precision_weighted: 0.6155 (+/- 0.0101)\n",
      "  f1_weighted: 0.6002 (+/- 0.0118)\n",
      "  f1_macro: 0.5834 (+/- 0.0215)\n",
      "  matthews: 0.5514 (+/- 0.0122)\n",
      "  balanced_accuracy: 0.5679 (+/- 0.0195)\n",
      "\n",
      "Neural Network (MLP):\n",
      "  accuracy: 0.4211 (+/- 0.0203)\n",
      "  recall_weighted: 0.4211 (+/- 0.0203)\n",
      "  precision_weighted: 0.4186 (+/- 0.0265)\n",
      "  f1_weighted: 0.4162 (+/- 0.0216)\n",
      "  f1_macro: 0.3704 (+/- 0.0384)\n",
      "  matthews: 0.3441 (+/- 0.0238)\n",
      "  balanced_accuracy: 0.3737 (+/- 0.0421)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ESTRATEGIA 1:\n",
    "print(\"=\"*80)\n",
    "print(\"ESTRATEGIA 1: Utilizar todas las variables, codificando categóricas con dummies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cat_cols1 = [\"exterior\", \"condition\", \"agency\", \"consumption_label\", \"emissions_label\"]\n",
    "\n",
    "X_zone_encoded_1 = pd.get_dummies(X_zone, columns=cat_cols1)\n",
    "# Eliminamos caracteres especiales de los nombres de las columnas que pueden hacer fallar algunos modelos\n",
    "X_zone_encoded_1.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_zone_encoded_1.columns]\n",
    "y_zone_encoded_1 = LabelEncoder().fit_transform(y_zone)\n",
    "\n",
    "print(f\"\\nNúmero de muestras: {len(X_zone_encoded_1)}\")\n",
    "print(f\"Número de features: {X_zone_encoded_1.shape[1]}\")\n",
    "print(f\"Distribución de clases: {np.bincount(y_zone_encoded_1)}\")\n",
    "\n",
    "result1 = evaluate_models(X_zone_encoded_1, y_zone_encoded_1, models=MODELS, cv_strategy=cv_strategy)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acf1f9",
   "metadata": {},
   "source": [
    "### Strategy 1.1\n",
    "We have seen that some models handle categorical columns automatically (without having to use the dummies), which can improve both the time required to fit the model and the result, so we will try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a2a07500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ESTRATEGIA 1.1: Utilizar todas las variables, manteniendo categóricas como categorías (modelos que lo soportan)\n",
      "================================================================================\n",
      "\n",
      "Número de muestras: 1230\n",
      "Número de features: 17\n",
      "Distribución de clases: [237 127  76  61  90 156 136  34 167  33 113]\n",
      "\n",
      "XGBoost:\n",
      "  accuracy: 0.6285 (+/- 0.0211)\n",
      "  recall_weighted: 0.6285 (+/- 0.0211)\n",
      "  precision_weighted: 0.6354 (+/- 0.0232)\n",
      "  f1_weighted: 0.6253 (+/- 0.0221)\n",
      "  f1_macro: 0.6213 (+/- 0.0228)\n",
      "  matthews: 0.5792 (+/- 0.0242)\n",
      "  balanced_accuracy: 0.6127 (+/- 0.0271)\n",
      "\n",
      "LightGBM:\n",
      "  accuracy: 0.6699 (+/- 0.0151)\n",
      "  recall_weighted: 0.6699 (+/- 0.0151)\n",
      "  precision_weighted: 0.6793 (+/- 0.0130)\n",
      "  f1_weighted: 0.6680 (+/- 0.0145)\n",
      "  f1_macro: 0.6538 (+/- 0.0222)\n",
      "  matthews: 0.6260 (+/- 0.0169)\n",
      "  balanced_accuracy: 0.6401 (+/- 0.0238)\n",
      "\n",
      "CatBoost:\n",
      "  accuracy: 0.5764 (+/- 0.0181)\n",
      "  recall_weighted: 0.5764 (+/- 0.0181)\n",
      "  precision_weighted: 0.5914 (+/- 0.0205)\n",
      "  f1_weighted: 0.5667 (+/- 0.0183)\n",
      "  f1_macro: 0.5343 (+/- 0.0277)\n",
      "  matthews: 0.5199 (+/- 0.0206)\n",
      "  balanced_accuracy: 0.5221 (+/- 0.0267)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Models that handle categorical features natively (we have tried class_weight='balanced' but results were very similar)\n",
    "MODELS_NATIVE = {\n",
    "    'XGBoost': XGBClassifier( n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1, tree_method=\"hist\", # <--- Necesario para el modo rápido\n",
    "        enable_categorical=True, use_label_encoder=False, eval_metric='mlogloss'), # <--- Habilita manejo de categóricas\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, n_jobs=-1, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=0, allow_writing_files=False,\n",
    "        cat_features=cat_cols1), # Evita crear carpetas 'catboost_info'\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ESTRATEGIA 1.1: Utilizar todas las variables, manteniendo categóricas como categorías (modelos que lo soportan)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_zone_unencoded_1 = X_zone.copy()\n",
    "\n",
    "# Eliminamos caracteres especiales de los nombres de las columnas que pueden hacer fallar algunos modelos\n",
    "X_zone_unencoded_1.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_zone_unencoded_1.columns]\n",
    "y_zone_unencoded_1 = LabelEncoder().fit_transform(y_zone)\n",
    "\n",
    "print(f\"\\nNúmero de muestras: {len(X_zone_unencoded_1)}\")\n",
    "print(f\"Número de features: {X_zone_unencoded_1.shape[1]}\")\n",
    "print(f\"Distribución de clases: {np.bincount(y_zone_unencoded_1)}\")\n",
    "\n",
    "result11 = evaluate_models(X_zone_unencoded_1, y_zone_unencoded_1, models=MODELS_NATIVE, scoring=SCORING)\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad92fb",
   "metadata": {},
   "source": [
    "### Strategy 1.2\n",
    "En la estrategia 1 no hemos podido usar SVM porque el tiempo de entrenamiento era demasiado largo, aun probando a ejecutarlo en la GPU en colab. Asi que vamos a reducir un poco la cantidad de dimensiones eliminando \"agency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models SVM\n",
    "MODELS_SVM = {\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ESTRATEGIA 1.2: Utilizar svm\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_zone_encoded_12 = X_zone.copy()\n",
    "X_zone_encoded_12.drop(columns=[\"agency\", \"consumption_label\", \"emissions_label\"], inplace=True)  # Eliminamos \"agency\" para reducir dimensionalidad\n",
    "\n",
    "cat_cols12 = [\"exterior\", \"condition\"]\n",
    "X_zone_encoded_12 = pd.get_dummies(X_zone_encoded_12, columns=cat_cols12)\n",
    "# Eliminamos caracteres especiales de los nombres de las columnas que pueden hacer fallar algunos modelos\n",
    "X_zone_encoded_12.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_zone_encoded_12.columns]\n",
    "\n",
    "y_zone_encoded_12 = LabelEncoder().fit_transform(y_zone)\n",
    "print(f\"\\nNúmero de muestras: {len(X_zone_encoded_12)}\")\n",
    "print(f\"Número de features: {X_zone_encoded_12.shape[1]}\")\n",
    "print(f\"Distribución de clases: {np.bincount(y_zone_encoded_12)}\")\n",
    "\n",
    "result12 = evaluate_models(X_zone_encoded_12, y_zone_encoded_12, models=MODELS_SVM, scoring=SCORING)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8188d6",
   "metadata": {},
   "source": [
    "### Strategy 1.3\n",
    "Vamos a probar los modelos pero con el argumento class_weight = balanced, ya que nuestras clases no estan balanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7c96b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ESTRATEGIA 1.3: utilizar los modelos con class_weight = balanced\n",
      "================================================================================\n",
      "\n",
      "Número de muestras: 1230\n",
      "Número de features: 324\n",
      "Distribución de clases: [237 127  76  61  90 156 136  34 167  33 113]\n",
      "\n",
      "Random Forest:\n",
      "  accuracy: 0.6024 (+/- 0.0206)\n",
      "  recall_weighted: 0.6024 (+/- 0.0206)\n",
      "  precision_weighted: 0.6330 (+/- 0.0247)\n",
      "  f1_weighted: 0.5918 (+/- 0.0216)\n",
      "  f1_macro: 0.5722 (+/- 0.0382)\n",
      "  matthews: 0.5476 (+/- 0.0236)\n",
      "  balanced_accuracy: 0.5435 (+/- 0.0350)\n",
      "\n",
      "Random Forest (tuned):\n",
      "  accuracy: 0.5862 (+/- 0.0191)\n",
      "  recall_weighted: 0.5862 (+/- 0.0191)\n",
      "  precision_weighted: 0.6101 (+/- 0.0294)\n",
      "  f1_weighted: 0.5860 (+/- 0.0232)\n",
      "  f1_macro: 0.5711 (+/- 0.0262)\n",
      "  matthews: 0.5337 (+/- 0.0225)\n",
      "  balanced_accuracy: 0.5765 (+/- 0.0280)\n",
      "\n",
      "Logistic Regression:\n",
      "  accuracy: 0.2333 (+/- 0.0278)\n",
      "  recall_weighted: 0.2333 (+/- 0.0278)\n",
      "  precision_weighted: 0.2551 (+/- 0.0240)\n",
      "  f1_weighted: 0.2306 (+/- 0.0250)\n",
      "  f1_macro: 0.1844 (+/- 0.0239)\n",
      "  matthews: 0.1582 (+/- 0.0303)\n",
      "  balanced_accuracy: 0.2177 (+/- 0.0274)\n",
      "\n",
      "Naive Bayes:\n",
      "  accuracy: 0.3317 (+/- 0.0208)\n",
      "  recall_weighted: 0.3317 (+/- 0.0208)\n",
      "  precision_weighted: 0.3318 (+/- 0.0481)\n",
      "  f1_weighted: 0.2834 (+/- 0.0243)\n",
      "  f1_macro: 0.2123 (+/- 0.0198)\n",
      "  matthews: 0.2463 (+/- 0.0258)\n",
      "  balanced_accuracy: 0.2459 (+/- 0.0209)\n",
      "\n",
      "Gradient Boosting:\n",
      "  accuracy: 0.5789 (+/- 0.0285)\n",
      "  recall_weighted: 0.5789 (+/- 0.0285)\n",
      "  precision_weighted: 0.5960 (+/- 0.0346)\n",
      "  f1_weighted: 0.5713 (+/- 0.0289)\n",
      "  f1_macro: 0.5425 (+/- 0.0405)\n",
      "  matthews: 0.5218 (+/- 0.0325)\n",
      "  balanced_accuracy: 0.5240 (+/- 0.0398)\n",
      "\n",
      "Hist Gradient Boosting:\n",
      "  accuracy: 0.6350 (+/- 0.0181)\n",
      "  recall_weighted: 0.6350 (+/- 0.0181)\n",
      "  precision_weighted: 0.6412 (+/- 0.0170)\n",
      "  f1_weighted: 0.6346 (+/- 0.0178)\n",
      "  f1_macro: 0.6169 (+/- 0.0200)\n",
      "  matthews: 0.5874 (+/- 0.0204)\n",
      "  balanced_accuracy: 0.6133 (+/- 0.0256)\n",
      "\n",
      "XGBoost:\n",
      "  accuracy: 0.6634 (+/- 0.0104)\n",
      "  recall_weighted: 0.6634 (+/- 0.0104)\n",
      "  precision_weighted: 0.6713 (+/- 0.0135)\n",
      "  f1_weighted: 0.6630 (+/- 0.0112)\n",
      "  f1_macro: 0.6572 (+/- 0.0121)\n",
      "  matthews: 0.6187 (+/- 0.0116)\n",
      "  balanced_accuracy: 0.6436 (+/- 0.0097)\n",
      "\n",
      "LightGBM:\n",
      "  accuracy: 0.6333 (+/- 0.0232)\n",
      "  recall_weighted: 0.6333 (+/- 0.0232)\n",
      "  precision_weighted: 0.6434 (+/- 0.0231)\n",
      "  f1_weighted: 0.6326 (+/- 0.0220)\n",
      "  f1_macro: 0.6133 (+/- 0.0182)\n",
      "  matthews: 0.5854 (+/- 0.0261)\n",
      "  balanced_accuracy: 0.6067 (+/- 0.0223)\n",
      "\n",
      "CatBoost:\n",
      "  accuracy: 0.6049 (+/- 0.0107)\n",
      "  recall_weighted: 0.6049 (+/- 0.0107)\n",
      "  precision_weighted: 0.6155 (+/- 0.0101)\n",
      "  f1_weighted: 0.6002 (+/- 0.0118)\n",
      "  f1_macro: 0.5834 (+/- 0.0215)\n",
      "  matthews: 0.5514 (+/- 0.0122)\n",
      "  balanced_accuracy: 0.5679 (+/- 0.0195)\n",
      "\n",
      "Neural Network (MLP):\n",
      "  accuracy: 0.4211 (+/- 0.0203)\n",
      "  recall_weighted: 0.4211 (+/- 0.0203)\n",
      "  precision_weighted: 0.4186 (+/- 0.0265)\n",
      "  f1_weighted: 0.4162 (+/- 0.0216)\n",
      "  f1_macro: 0.3704 (+/- 0.0384)\n",
      "  matthews: 0.3441 (+/- 0.0238)\n",
      "  balanced_accuracy: 0.3737 (+/- 0.0421)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Probar múltiples modelos\n",
    "MODELS_BALANCED = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Random Forest (tuned)': RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_split=5, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial', class_weight='balanced'),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "#    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True),\n",
    "#    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingClassifier(max_iter=100, random_state=42, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=0, allow_writing_files=False), # (Verbose=0 para que no llene la pantalla de logs)\n",
    "    'Neural Network (MLP)': make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "}\n",
    "\n",
    "\n",
    "# ESTRATEGIA 1.3:\n",
    "print(\"=\"*80)\n",
    "print(\"ESTRATEGIA 1.3: utilizar los modelos con class_weight = balanced\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nNúmero de muestras: {len(X_zone_encoded_1)}\")\n",
    "print(f\"Número de features: {X_zone_encoded_1.shape[1]}\")\n",
    "print(f\"Distribución de clases: {np.bincount(y_zone_encoded_1)}\")\n",
    "\n",
    "result13 = evaluate_models(X_zone_encoded_1, y_zone_encoded_1, models=MODELS_BALANCED, cv_strategy=cv_strategy)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183222b",
   "metadata": {},
   "source": [
    "### Step 1 conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4d08d2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Tuning de LightGBM...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      "Mejor Score (F1 Macro): 0.5709\n",
      "Mejores Parámetros: {'colsample_bytree': np.float64(0.845366078374316), 'learning_rate': np.float64(0.09364860725812378), 'max_depth': 10, 'min_child_samples': 39, 'n_estimators': 341, 'num_leaves': 24, 'reg_alpha': np.float64(0.26366974497252005), 'reg_lambda': np.float64(3.7646336687804958), 'subsample': np.float64(0.9242213323127332)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# 1. Definir el modelo base\n",
    "lgbm = LGBMClassifier(\n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    class_weight='balanced',  # Importante por tu desbalance\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# 2. Definir el espacio de búsqueda (Grid)\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),      # Número de árboles\n",
    "    'learning_rate': uniform(0.01, 0.2),     # Velocidad de aprendizaje\n",
    "    'num_leaves': randint(20, 100),          # Complejidad del árbol (vital en LGBM)\n",
    "    'max_depth': randint(3, 15),             # Profundidad máxima\n",
    "    'min_child_samples': randint(10, 100),   # Mínimo de datos por hoja (evita overfitting)\n",
    "    'subsample': uniform(0.6, 0.4),          # Usar solo parte de los datos por árbol\n",
    "    'colsample_bytree': uniform(0.6, 0.4),   # Usar solo parte de las features por árbol\n",
    "    'reg_alpha': uniform(0, 10),             # Regularización L1 (importante para datos ruidosos)\n",
    "    'reg_lambda': uniform(0, 10),            # Regularización L2\n",
    "}\n",
    "\n",
    "# 3. Configurar la búsqueda\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,              # Probará 100 combinaciones (tardará unos minutos)\n",
    "    scoring='f1_macro',      # Optimizamos para f1_macro (clases pequeñas)\n",
    "    cv=5,                    # StratifiedKFold de 5 splits\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 4. Entrenar (Asegúrate de pasar X con dtype='category')\n",
    "print(\"Iniciando Tuning de LightGBM...\")\n",
    "search.fit(X_zone_unencoded_1, y_zone_unencoded_1) # Usa tus variables nativas (17 features)\n",
    "\n",
    "# 5. Resultados\n",
    "print(f\"\\nMejor Score (F1 Macro): {search.best_score_:.4f}\")\n",
    "print(f\"Mejores Parámetros: {search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
