{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8f242f",
   "metadata": {},
   "source": [
    "# **SCRAPER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfadcf",
   "metadata": {},
   "source": [
    "## **Initializing the scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#: Import the libraries for the scraping\n",
    "import undetected_chromedriver as uc  # type: ignore\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "#! FUNCTIONS:\n",
    "#! Function to initialize the Chrome driver\n",
    "def init_driver() -> uc.Chrome:\n",
    "    options = uc.ChromeOptions()\n",
    "\n",
    "    #options.add_argument('--headless')\n",
    "    #options.add_argument('--no-sandbox')\n",
    "    # options.add_argument('--disable-dev-shm-usage')\n",
    "    # options.add_argument('--disable-gpu')\n",
    "    # options.add_argument('--disable-extensions')\n",
    "    # options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    \n",
    "    driver = uc.Chrome(options=options)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9261771",
   "metadata": {},
   "source": [
    "## **Scraping house url**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8999d4c",
   "metadata": {},
   "source": [
    "### List with all zones in Bilbao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distrits of Bilbao are: Deusto, Uribarri, Otxarkoaga/Txurdinaga, \n",
    "#                              Basurto/Zorroza, Abando, Begoña, \n",
    "#                              Rekalde, e Ibaiondo \n",
    "\n",
    "zonas = {\"deusto\":[97, []], \"uribarri\":[136, []], \"otxarkoaga-txurdinaga\":[36, []],\n",
    "         \"basurto-zorroza\":[151, []], \"abando-albia\":[275, []], \"indautxu\":[164, []], \"casco-viejo\":[65, []], \n",
    "         \"begona-santutxu\":[85, []], \"rekalde\":[213, []], \"ibaiondo\":[212, []], \"san-adrian-la-pena\":[44, []]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57f5b6",
   "metadata": {},
   "source": [
    "### Scraping logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ebe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add checked areas beacause sometimes we are banned from the web page, so we don't start again from the beginning\n",
    "zonas_miradas = [\"deusto\", \"uribarri\", \"otxarkoaga-txurdinaga\", \"basurto-zorroza\", \"abando-albia\", \n",
    "                 \"indautxu\", \"casco-viejo\", \"rekalde\", \"ibaiondo\", \"san-adrian-la-pena\"]\n",
    "\n",
    "driver = init_driver()\n",
    "\n",
    "for ubicacion in zonas:\n",
    "    if ubicacion in zonas_miradas: continue\n",
    "    for page in range(1, (zonas[ubicacion][0] // 30) + 2): # +2 porque +1 para que llegue al valor y +1 para que mire tambien el resto\n",
    "\n",
    "        url_base = f\"https://www.idealista.com/venta-viviendas/bilbao/{ubicacion}/pagina-{page}.htm\"\n",
    "\n",
    "        driver.get(url_base)\n",
    "\n",
    "        sleep(10)\n",
    "\n",
    "        # 1. Sacar enlaces de cada anuncio\n",
    "        links = driver.find_elements(By.CSS_SELECTOR, \"a.item-link\")\n",
    "        urls_detalle = [a.get_attribute(\"href\") for a in links]\n",
    "\n",
    "        zonas[ubicacion][1].extend(urls_detalle)\n",
    "        \n",
    "        sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fb5e0",
   "metadata": {},
   "source": [
    "### Test completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1610fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la ubicacion: deusto hay 97 casas y hay 0 urls\n",
      "En la ubicacion: uribarri hay 136 casas y hay 0 urls\n",
      "En la ubicacion: otxarkoaga-txurdinaga hay 36 casas y hay 0 urls\n",
      "En la ubicacion: basurto-zorroza hay 151 casas y hay 0 urls\n",
      "En la ubicacion: abando-albia hay 275 casas y hay 0 urls\n",
      "En la ubicacion: indautxu hay 164 casas y hay 0 urls\n",
      "En la ubicacion: casco-viejo hay 65 casas y hay 0 urls\n",
      "En la ubicacion: begona-santutxu hay 85 casas y hay 88 urls\n",
      "En la ubicacion: rekalde hay 213 casas y hay 0 urls\n",
      "En la ubicacion: ibaiondo hay 212 casas y hay 0 urls\n",
      "En la ubicacion: san-adrian-la-pena hay 44 casas y hay 0 urls\n"
     ]
    }
   ],
   "source": [
    "# Comprobar que tenemos todas las casa\n",
    "for ubicacion in zonas:\n",
    "    print(f\"En la ubicacion: {ubicacion} hay {zonas[ubicacion][0]} casas y hay {len(zonas[ubicacion][1])} urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0193e0f",
   "metadata": {},
   "source": [
    "### Safe data into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV generado: idealista_urls.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = \"../data/raw/idealista_urls.csv\"\n",
    "\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"zona\", \"url\"])  # cabecera\n",
    "\n",
    "    for zona, (n_total, urls) in zonas.items():\n",
    "        for url in urls:\n",
    "            writer.writerow([zona, url])\n",
    "\n",
    "print(f\"CSV generado: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3917b0",
   "metadata": {},
   "source": [
    "## **Scraping house details**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f7b9d",
   "metadata": {},
   "source": [
    "###  Reading from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269526bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deusto', 'https://www.idealista.com/inmueble/109356873/'], ['deusto', 'https://www.idealista.com/inmueble/106221410/'], ['deusto', 'https://www.idealista.com/inmueble/107750109/'], ['deusto', 'https://www.idealista.com/inmueble/106221526/'], ['deusto', 'https://www.idealista.com/inmueble/108491309/']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def csv_a_lista(ruta_csv):\n",
    "    lista = []\n",
    "    with open(ruta_csv, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for fila in reader:\n",
    "            if fila:  # evitar líneas vacías\n",
    "                lista.append([fila[0], fila[1]])\n",
    "    return lista\n",
    "\n",
    "# Ejemplo de uso\n",
    "lista_zona_url = csv_a_lista(\"../data/raw/idealista_urls.csv\")\n",
    "\n",
    "print(lista_zona_url[:5])  # muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54353be",
   "metadata": {},
   "source": [
    "### Sraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e678348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class PropertyScraper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract_property_data_from_driver(self, driver) -> Dict:\n",
    "        \"\"\"\n",
    "        Extrae toda la información de una propiedad desde el driver de Selenium\n",
    "        \"\"\"\n",
    "        html_content = driver.page_source\n",
    "        return self.extract_property_data(html_content)\n",
    "\n",
    "    def extract_property_data(self, html_content: str) -> Dict:\n",
    "        \"\"\"f\n",
    "        Extrae toda la información de una propiedad desde el HTML\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        property_data = {\n",
    "            'url': None,\n",
    "            'precio': None,\n",
    "            'zona': None,\n",
    "            'barrio': None,\n",
    "            'm2_construidos': None,\n",
    "            'm2_utiles': None,\n",
    "            'n_habitaciones': None,\n",
    "            'n_baños': None,\n",
    "            'n_planta': None,\n",
    "            'exterior_interior': None,\n",
    "            'ascensor': None,\n",
    "            'garaje': None,\n",
    "            'trastero': None,\n",
    "            'balcon': None,\n",
    "            'obra_nueva_segunda_mano': None,\n",
    "            'estado': None,\n",
    "            'año_construccion': None,\n",
    "            'profesional': None,\n",
    "            'calefaccion': None,\n",
    "            'consumo_valor': None,\n",
    "            'consumo_etiqueta': None,\n",
    "            'emisiones_valor': None,\n",
    "            'emisiones_etiqueta': None,\n",
    "            'descripcion': None\n",
    "        }\n",
    "\n",
    "        # Extraer barrio\n",
    "        property_data['barrio'] = self._extract_neighborhood(soup)\n",
    "        \n",
    "        # Extraer descripción\n",
    "        property_data['descripcion'] = self._extract_description(soup)\n",
    "        \n",
    "        # Extraer precios\n",
    "        property_data['precio'] = self._extract_price(soup)\n",
    "\n",
    "        # Extraer profesional/inmobiliaria\n",
    "        property_data['profesional'] = self._extract_professional(soup)\n",
    "        \n",
    "        # Extraer información de .info-features\n",
    "        self._extract_info_features(soup, property_data)\n",
    "\n",
    "        # Extraer número de planta desde la referencia (si existe)\n",
    "        self._extract_floor_from_reference(soup, property_data)\n",
    "        \n",
    "        # Extraer información detallada de las listas\n",
    "        self._extract_detailed_features(soup, property_data)\n",
    "        \n",
    "        # Extraer certificado energético\n",
    "        self._extract_energy_certificate(soup, property_data)\n",
    "        \n",
    "        return property_data\n",
    "    \n",
    "    def _extract_professional(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"Extrae el nombre del profesional/inmobiliaria\"\"\"\n",
    "        professional_div = soup.find('div', class_='professional-name')\n",
    "        if professional_div:\n",
    "            tipo_profesiona_div = soup.find('div', class_= 'name').get_text(strip=True)\n",
    "            # Buscar el span que contiene el nombre\n",
    "            span = professional_div.find('span')\n",
    "            if span and tipo_profesiona_div != \"Particular\":\n",
    "                # Extraer el texto directamente del span (sin los inputs)\n",
    "                name = span.get_text(strip=True)\n",
    "                # Limpiar espacios múltiples\n",
    "                name = re.sub(r'\\s+', ' ', name)\n",
    "                return name if name else None\n",
    "        return None\n",
    "    \n",
    "    def _extract_description(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"Extrae la descripción de la propiedad\"\"\"\n",
    "        comment_div = soup.find('div', class_='comment')\n",
    "        if comment_div:\n",
    "            # Buscar el div con la clase adCommentsLanguage que contiene el párrafo\n",
    "            ad_comment = comment_div.find('div', class_='adCommentsLanguage')\n",
    "            if ad_comment:\n",
    "                # Extraer el texto del párrafo\n",
    "                p_element = ad_comment.find('p')\n",
    "                if p_element:\n",
    "                    # Obtener el texto y reemplazar <br> por espacios\n",
    "                    description = p_element.get_text(separator=' ', strip=True)\n",
    "                    # Limpiar espacios múltiples\n",
    "                    description = re.sub(r'\\s+', ' ', description)\n",
    "                    return description\n",
    "        return None\n",
    "    \n",
    "    def _extract_neighborhood(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"Extrae el barrio de la propiedad\"\"\"\n",
    "        neighborhood_element = soup.find('span', class_='main-info__title-minor')\n",
    "        if neighborhood_element:\n",
    "            return neighborhood_element.get_text(strip=True)\n",
    "        return None\n",
    "    \n",
    "    def _extract_price(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"Extrae el precio de la propiedad\"\"\"\n",
    "        price_element = soup.find('span', class_='info-data-price')\n",
    "        if price_element:\n",
    "            price_text = price_element.get_text(strip=True)\n",
    "            # Extraer solo los números y puntos del precio\n",
    "            price_match = re.search(r'([\\d.]+)', price_text.replace('€', ''))\n",
    "            return price_match.group(1) if price_match else None\n",
    "        return None\n",
    "    \n",
    "    def _extract_floor_from_reference(self, soup: BeautifulSoup, data: Dict):\n",
    "        \"\"\"Extrae el número de planta desde la referencia del anuncio\"\"\"\n",
    "        # Solo extraer si aún no tenemos el número de planta\n",
    "        if data['n_planta'] is not None:\n",
    "            return\n",
    "        \n",
    "        ad_reference_div = soup.find('div', class_='ad-reference-container')\n",
    "        if ad_reference_div:\n",
    "            txt_ref = ad_reference_div.find('p', class_='txt-ref')\n",
    "            if txt_ref:\n",
    "                ref_text = txt_ref.get_text(strip=True)\n",
    "                # Buscar patrón como \"2 1ºA\" donde el número antes de º es la planta\n",
    "                floor_match = re.search(r'(\\d+)º', ref_text)\n",
    "                if floor_match:\n",
    "                    data['n_planta'] = floor_match.group(1)\n",
    "    \n",
    "    def _extract_info_features(self, soup: BeautifulSoup, data: Dict):\n",
    "        \"\"\"Extrae información básica del div info-features\"\"\"\n",
    "        info_features = soup.find('div', class_='info-features')\n",
    "        if not info_features:\n",
    "            return\n",
    "        \n",
    "        spans = info_features.find_all('span')\n",
    "        \n",
    "        for span in spans:\n",
    "            text = span.get_text(strip=True)\n",
    "            \n",
    "            # Obra nueva\n",
    "            if 'Obra nueva' in text:\n",
    "                data['obra_nueva_segunda_mano'] = 'Obra nueva'\n",
    "            \n",
    "            # Metros cuadrados\n",
    "            if 'm²' in text:\n",
    "                m2_match = re.search(r'(\\d+)\\s*m²', text)\n",
    "                if m2_match and not data['m2_construidos']:\n",
    "                    data['m2_construidos'] = m2_match.group(1)\n",
    "            \n",
    "            # Habitaciones\n",
    "            if 'hab.' in text:\n",
    "                hab_match = re.search(r'(\\d+)\\s*hab\\.', text)\n",
    "                if hab_match:\n",
    "                    data['n_habitaciones'] = hab_match.group(1)\n",
    "            \n",
    "            # Planta y características del edificio\n",
    "            if 'Planta' in text:\n",
    "                # Extraer número de planta\n",
    "                planta_match = re.search(r'Planta\\s+(\\d+)ª', text)\n",
    "                if planta_match:\n",
    "                    data['n_planta'] = planta_match.group(1)\n",
    "                \n",
    "                # Exterior/Interior\n",
    "                if 'exterior' in text.lower():\n",
    "                    data['exterior_interior'] = 'exterior'\n",
    "                elif 'interior' in text.lower():\n",
    "                    data['exterior_interior'] = 'interior'\n",
    "                \n",
    "                # Ascensor\n",
    "                if 'con ascensor' in text.lower():\n",
    "                    data['ascensor'] = 'Sí'\n",
    "                elif 'sin ascensor' in text.lower():\n",
    "                    data['ascensor'] = 'No'\n",
    "            \n",
    "            # Garaje\n",
    "            if 'Garaje incluido' in text:\n",
    "                data['garaje'] = 'Incluido'\n",
    "    \n",
    "    def _extract_detailed_features(self, soup: BeautifulSoup, data: Dict):\n",
    "        \"\"\"Extrae información detallada de las listas ul\"\"\"\n",
    "        # Buscar todos los divs con clase 'details-property'\n",
    "        details_divs = soup.find_all('div', class_='details-property')\n",
    "        \n",
    "        if not details_divs:\n",
    "            return\n",
    "        \n",
    "        # Filtrar para obtener solo el div correcto (el que tiene las características)\n",
    "        # Evitar el div con clase 'profile-qualification-homes-info'\n",
    "        target_div = None\n",
    "        for div in details_divs:\n",
    "            # Verificar que no tenga la clase 'profile-qualification-homes-info'\n",
    "            if 'profile-qualification-homes-info' not in div.get('class', []):\n",
    "                target_div = div\n",
    "                break\n",
    "        \n",
    "        if not target_div:\n",
    "            return\n",
    "        \n",
    "        # Buscar todas las listas dentro del div correcto\n",
    "        ul_elements = target_div.find_all('ul')\n",
    "        \n",
    "        for ul in ul_elements:\n",
    "            li_elements = ul.find_all('li')\n",
    "            \n",
    "            for li in li_elements:\n",
    "                text = li.get_text(strip=True)\n",
    "                self._parse_feature_text(text, data)\n",
    "    \n",
    "    def _parse_feature_text(self, text: str, data: Dict):\n",
    "        \"\"\"Analiza el texto de cada característica y extrae la información\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Metros cuadrados construidos/útiles\n",
    "        if 'm²' in text:\n",
    "            # Buscar patrón: \"67 m² construidos, 65 m² útiles\"\n",
    "            construidos_match = re.search(r'(\\d+)\\s*m²\\s+construidos', text)\n",
    "            utiles_match = re.search(r'(\\d+)\\s*m²\\s+útiles', text)\n",
    "            \n",
    "            if construidos_match:\n",
    "                data['m2_construidos'] = construidos_match.group(1)\n",
    "            if utiles_match:\n",
    "                data['m2_utiles'] = utiles_match.group(1)\n",
    "            \n",
    "            # Si no hay especificación \"construidos\" o \"útiles\", asumir construidos\n",
    "            if not construidos_match and not utiles_match and not data['m2_construidos']:\n",
    "                m2_match = re.search(r'(\\d+)\\s*m²', text)\n",
    "                if m2_match:\n",
    "                    data['m2_construidos'] = m2_match.group(1)\n",
    "        \n",
    "        # Habitaciones\n",
    "        if 'habitaciones' in text_lower or 'habitación' in text_lower:\n",
    "            hab_match = re.search(r'(\\d+)', text)\n",
    "            if hab_match:\n",
    "                data['n_habitaciones'] = hab_match.group(1)\n",
    "        \n",
    "        # Baños\n",
    "        if 'baños' in text_lower or 'baño' in text_lower:\n",
    "            baño_match = re.search(r'(\\d+)', text)\n",
    "            if baño_match:\n",
    "                data['n_baños'] = baño_match.group(1)\n",
    "        \n",
    "        # Balcón/Terraza\n",
    "        if 'balcón' in text_lower:\n",
    "            data['balcon'] = 'Sí'\n",
    "        elif 'terraza' in text_lower:\n",
    "            if not data['balcon']:\n",
    "                data['balcon'] = 'Terraza'\n",
    "            else:\n",
    "                data['balcon'] = 'Balcón y terraza'\n",
    "        \n",
    "        # Garaje\n",
    "        if 'plaza de garaje' in text_lower or 'garaje' in text_lower:\n",
    "            if 'incluida' in text_lower or 'incluido' in text_lower:\n",
    "                data['garaje'] = 'Incluido'\n",
    "            else:\n",
    "                data['garaje'] = 'Sí'\n",
    "        \n",
    "        # Estado de la vivienda\n",
    "        if 'segunda mano' in text_lower:\n",
    "            data['obra_nueva_segunda_mano'] = 'Segunda mano'\n",
    "            if 'buen estado' in text_lower:\n",
    "                data['estado'] = 'Buen estado'\n",
    "            elif 'reformar' in text_lower:\n",
    "                data['estado'] = 'A reformar'\n",
    "        elif 'obra nueva' in text_lower or 'promoción de obra nueva' in text_lower:\n",
    "            data['obra_nueva_segunda_mano'] = 'Obra nueva'\n",
    "        \n",
    "        # Trastero\n",
    "        if 'trastero' in text_lower:\n",
    "            data['trastero'] = 'Sí'\n",
    "\n",
    "        # Calefacción (solo individual o central)\n",
    "        if 'calefacción' in text_lower or 'calefaccion' in text_lower:\n",
    "            if 'individual' in text_lower:\n",
    "                data['calefaccion'] = 'Individual'\n",
    "            elif 'central' in text_lower:\n",
    "                data['calefaccion'] = 'Central'\n",
    "        \n",
    "        # Año de construcción\n",
    "        if 'construido en' in text_lower:\n",
    "            año_match = re.search(r'(\\d{4})', text)\n",
    "            if año_match:\n",
    "                data['año_construccion'] = año_match.group(1)\n",
    "        \n",
    "        # Planta (desde edificio)\n",
    "        if 'planta' in text_lower and 'ª' in text:\n",
    "            planta_match = re.search(r'(\\d+)ª', text)\n",
    "            if planta_match:\n",
    "                data['n_planta'] = planta_match.group(1)\n",
    "            \n",
    "            if 'exterior' in text_lower:\n",
    "                data['exterior_interior'] = 'exterior'\n",
    "            elif 'interior' in text_lower:\n",
    "                data['exterior_interior'] = 'interior'\n",
    "        \n",
    "        # Ascensor\n",
    "        if 'con ascensor' in text_lower:\n",
    "            data['ascensor'] = 'Sí'\n",
    "        elif 'sin ascensor' in text_lower:\n",
    "            data['ascensor'] = 'No'\n",
    "    \n",
    "    def _extract_energy_certificate(self, soup: BeautifulSoup, data: Dict):\n",
    "        \"\"\"Extrae información del certificado energético\"\"\"\n",
    "        # Buscar la sección del certificado energético\n",
    "        energy_section = soup.find('h2', string=re.compile(r'Certificado energético'))\n",
    "        if not energy_section:\n",
    "            return\n",
    "        \n",
    "        # Buscar el div padre que contiene la información energética\n",
    "        energy_div = energy_section.find_next('div', class_='details-property_features')\n",
    "        if not energy_div:\n",
    "            return\n",
    "        \n",
    "        ul_element = energy_div.find('ul')\n",
    "        if not ul_element:\n",
    "            return\n",
    "        \n",
    "        li_elements = ul_element.find_all('li')\n",
    "        \n",
    "        for li in li_elements:\n",
    "            text = li.get_text(strip=True)\n",
    "            \n",
    "            # Consumo\n",
    "            if 'Consumo:' in text:\n",
    "                consumo_span = li.find('span', class_=re.compile(r'icon-energy-c-'))\n",
    "                if consumo_span:\n",
    "                    consumo_text = consumo_span.get_text(strip=True)\n",
    "                    if consumo_text:\n",
    "                        # Extraer valor numérico\n",
    "                        valor_match = re.search(r'([\\d,]+(?:\\.\\d+)?)', consumo_text)\n",
    "                        if valor_match:\n",
    "                            data['consumo_valor'] = valor_match.group(1)\n",
    "                    \n",
    "                    # Extraer etiqueta energética de la clase\n",
    "                    class_list = consumo_span.get('class', [])\n",
    "                    for class_name in class_list:\n",
    "                        if 'icon-energy-c-' in class_name:\n",
    "                            etiqueta = class_name.split('-')[-1].upper()\n",
    "                            data['consumo_etiqueta'] = etiqueta\n",
    "            \n",
    "            # Emisiones\n",
    "            elif 'Emisiones:' in text:\n",
    "                emisiones_span = li.find('span', class_=re.compile(r'icon-energy-c-'))\n",
    "                if emisiones_span:\n",
    "                    emisiones_text = emisiones_span.get_text(strip=True)\n",
    "                    if emisiones_text:\n",
    "                        # Extraer valor numérico\n",
    "                        valor_match = re.search(r'([\\d,]+(?:\\.\\d+)?)', emisiones_text)\n",
    "                        if valor_match:\n",
    "                            data['emisiones_valor'] = valor_match.group(1)\n",
    "                    \n",
    "                    # Extraer etiqueta energética de la clase\n",
    "                    class_list = emisiones_span.get('class', [])\n",
    "                    for class_name in class_list:\n",
    "                        if 'icon-energy-c-' in class_name:\n",
    "                            etiqueta = class_name.split('-')[-1].upper()\n",
    "                            data['emisiones_etiqueta'] = etiqueta\n",
    "    \n",
    "\n",
    "    #! FUNCION QUE EMPIEZA EL CICLO DEL SCRAPING (SIN GUARDAR EN CSV)\n",
    "    def scrape_properties_from_zone_urls(self, driver, zone_url_pairs: list) -> list:\n",
    "        \"\"\"\n",
    "        Hace scraping de múltiples URLs con información de zona usando el driver de Selenium\n",
    "        \n",
    "        Args:\n",
    "            driver: Instancia del driver de Selenium ya inicializada\n",
    "            zone_url_pairs: Lista de listas en formato [[zona1, url1], [zona1, url2], [zona2, url3], ...]\n",
    "            \n",
    "        Returns:\n",
    "            Lista de diccionarios con los datos de cada propiedad incluyendo la zona\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, (zona, url) in enumerate(zone_url_pairs):\n",
    "            try:\n",
    "                print(f\"Procesando propiedad {i+1}/{len(zone_url_pairs)} - Zona: {zona} - URL: {url}\")\n",
    "                \n",
    "                # Navegar a la URL\n",
    "                driver.get(url)\n",
    "                \n",
    "                # Esperar un poco para que cargue la página (opcional)\n",
    "                sleep(10)\n",
    "                \n",
    "                # Extraer datos\n",
    "                property_data = self.extract_property_data_from_driver(driver)\n",
    "                property_data['zona'] = zona  # Añadir la zona recibida por parámetro\n",
    "                property_data['url'] = url    # Añadir la URL para referencia\n",
    "                \n",
    "                results.append(property_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {url} (Zona: {zona}): {e}\")\n",
    "                # Añadir entrada vacía para mantener el orden\n",
    "                results.append({\n",
    "                    'url': None,\n",
    "                    'precio': None,\n",
    "                    'zona': None,\n",
    "                    'barrio': None,\n",
    "                    'm2_construidos': None,\n",
    "                    'm2_utiles': None,\n",
    "                    'n_habitaciones': None,\n",
    "                    'n_baños': None,\n",
    "                    'n_planta': None,\n",
    "                    'exterior_interior': None,\n",
    "                    'ascensor': None,\n",
    "                    'garaje': None,\n",
    "                    'trastero': None,\n",
    "                    'balcon': None,\n",
    "                    'obra_nueva_segunda_mano': None,\n",
    "                    'estado': None,\n",
    "                    'año_construccion': None,\n",
    "                    'profesional': None,\n",
    "                    'calefaccion': None,\n",
    "                    'consumo_valor': None,\n",
    "                    'consumo_etiqueta': None,\n",
    "                    'emisiones_valor': None,\n",
    "                    'emisiones_etiqueta': None,\n",
    "                    'descripcion': None,\n",
    "                    'error': str(e)\n",
    "                    })\n",
    "        return results\n",
    "    \n",
    "\n",
    "    #! FUNCION QUE EMPIEZA EL CICLO DEL SCRAPING (GUARDANDO EN CSV)\n",
    "    def scrape_properties_from_zone_urls_with_csv(self, driver, zone_url_pairs: list, csv_filename: str = 'propiedades_scraping.csv') -> list:\n",
    "        \"\"\"\n",
    "        Hace scraping de múltiples URLs guardando cada propiedad inmediatamente en CSV\n",
    "        \n",
    "        Args:\n",
    "            driver: Instancia del driver de Selenium ya inicializada\n",
    "            zone_url_pairs: Lista de listas en formato [[zona1, url1], [zona1, url2], [zona2, url3], ...]\n",
    "            csv_filename: Nombre del archivo CSV donde guardar los datos\n",
    "            \n",
    "        Returns:\n",
    "            Lista de diccionarios con los datos de cada propiedad incluyendo la zona\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Definir las columnas del CSV\n",
    "        fieldnames = [\n",
    "            'url', 'precio', 'zona', 'barrio',\n",
    "            'm2_construidos', 'm2_utiles',\n",
    "            'n_habitaciones', 'n_baños', 'n_planta',\n",
    "            'exterior_interior', 'ascensor', 'garaje',\n",
    "            'trastero', 'balcon', 'obra_nueva_segunda_mano', \n",
    "            'estado', 'año_construccion', 'calefaccion', \n",
    "            'profesional', 'consumo_valor', 'consumo_etiqueta',\n",
    "            'emisiones_valor', 'emisiones_etiqueta',\n",
    "            'descripcion', 'error'\n",
    "        ]\n",
    "\n",
    "        \n",
    "        # Verificar si el archivo existe para saber si escribir headers\n",
    "        file_exists = os.path.isfile(csv_filename)\n",
    "        \n",
    "        # Abrir el archivo en modo append\n",
    "        with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Escribir headers solo si el archivo es nuevo\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            for i, (zona, url) in enumerate(zone_url_pairs):\n",
    "                try:\n",
    "                    print(f\"Procesando propiedad {i+1}/{len(zone_url_pairs)} - Zona: {zona} - URL: {url}\")\n",
    "                    \n",
    "                    # Navegar a la URL\n",
    "                    driver.get(url)\n",
    "                    \n",
    "                    # Esperar un poco para que cargue la página\n",
    "                    import time\n",
    "                    time.sleep(10)\n",
    "                    \n",
    "                    # Extraer datos\n",
    "                    property_data = self.extract_property_data_from_driver(driver)\n",
    "                    property_data['url'] = url\n",
    "                    property_data['zona'] = zona\n",
    "                    property_data['error'] = None\n",
    "                    \n",
    "                    # Guardar inmediatamente en CSV\n",
    "                    writer.writerow(property_data)\n",
    "                    csvfile.flush()  # Forzar escritura al disco\n",
    "                    \n",
    "                    results.append(property_data)\n",
    "                    print(f\"✓ Guardada en CSV: {url}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error procesando {url} (Zona: {zona}): {e}\")\n",
    "                    \n",
    "                    # Guardar el error también en CSV\n",
    "                    error_data = {\n",
    "                        'url': None,\n",
    "                        'precio': None,\n",
    "                        'zona': None,\n",
    "                        'barrio': None,\n",
    "                        'm2_construidos': None,\n",
    "                        'm2_utiles': None,\n",
    "                        'n_habitaciones': None,\n",
    "                        'n_baños': None,\n",
    "                        'n_planta': None,\n",
    "                        'exterior_interior': None,\n",
    "                        'ascensor': None,\n",
    "                        'garaje': None,\n",
    "                        'trastero': None,\n",
    "                        'balcon': None,\n",
    "                        'obra_nueva_segunda_mano': None,\n",
    "                        'estado': None,\n",
    "                        'año_construccion': None,\n",
    "                        'profesional': None,\n",
    "                        'calefaccion': None,\n",
    "                        'consumo_valor': None,\n",
    "                        'consumo_etiqueta': None,\n",
    "                        'emisiones_valor': None,\n",
    "                        'emisiones_etiqueta': None,\n",
    "                        'descripcion': None,\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "                    \n",
    "                    writer.writerow(error_data)\n",
    "                    csvfile.flush()\n",
    "                    results.append(error_data)\n",
    "        \n",
    "        print(f\"\\n✓ Scraping completado. Datos guardados en: {csv_filename}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75af590",
   "metadata": {},
   "source": [
    "### Functions to start scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f491d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que crea el objeto PropertyScraper y hace que empiece el scraping\n",
    "def scrape_properties_with_zones_selenium(driver, zone_url_pairs):\n",
    "    \"\"\"\n",
    "    Función para hacer scraping de propiedades con información de zona usando Selenium\n",
    "    \n",
    "    Args:\n",
    "        driver: Driver de Selenium inicializado\n",
    "        zone_url_pairs: Lista de listas en formato [[zona1, url1], [zona1, url2], [zona2, url3], ...]\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con todos los datos extraídos incluyendo zona\n",
    "    \"\"\"\n",
    "    scraper = PropertyScraper()\n",
    "    return scraper.scrape_properties_from_zone_urls(driver, zone_url_pairs)\n",
    "\n",
    "# Funcion que hace lo mismo que la anterios pero guarda datos en csv\n",
    "def scrape_properties_with_zones_selenium_to_csv(driver, zone_url_pairs, csv_filename='propiedades_scraping.csv'):\n",
    "    \"\"\"\n",
    "    Función para hacer scraping de propiedades guardando cada una inmediatamente en CSV\n",
    "    \n",
    "    Args:\n",
    "        driver: Driver de Selenium inicializado\n",
    "        zone_url_pairs: Lista de listas en formato [[zona1, url1], [zona1, url2], [zona2, url3], ...]\n",
    "        csv_filename: Nombre del archivo CSV (por defecto 'propiedades_scraping.csv')\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con todos los datos extraídos\n",
    "    \"\"\"\n",
    "    scraper = PropertyScraper()\n",
    "    return scraper.scrape_properties_from_zone_urls_with_csv(driver, zone_url_pairs, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84728e8",
   "metadata": {},
   "source": [
    "### Scraping main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb47568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso con Selenium:\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Tu driver ya inicializado\n",
    "    driver = init_driver()  # o el que uses\n",
    "    \n",
    "    \n",
    "    #? Comentamos este metodo porque no guarda la informacion en csv\n",
    "    #all_data = scrape_properties_with_zones_selenium(driver, lista_zona_url)\n",
    "\n",
    "    # Scraping con zonas y guardando en csv\n",
    "    all_data = scrape_properties_with_zones_selenium_to_csv(driver, lista_zona_url, '../data/raw/idealista_viviendas_detalle.csv')\n",
    "\n",
    "    # Guardar resultados\n",
    "    # import pandas as pd\n",
    "    # df = pd.DataFrame(all_data)\n",
    "    # df.to_csv('propiedades_scrapeadas.csv', index=False)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "back_market",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
